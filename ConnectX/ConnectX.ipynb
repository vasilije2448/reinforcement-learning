{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for testing and commentary.\n",
    "\n",
    "For each model, there's a .py file which was used for its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from kaggle_environments import make, evaluate\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from OneStepNegamaxQLearning import AgentDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(\".\",\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides board into 3 channels - https://www.kaggle.com/c/connectx/discussion/168246\n",
    "# first channel: player 1 pieces\n",
    "# second channel: player 2 pieces\n",
    "# third channel: possible moves. 1 for player_1 and -1 for player_2\n",
    "def transform_board(board, mark):\n",
    "    rows = board[0].shape[0]\n",
    "    columns = board[0].shape[1]\n",
    "\n",
    "    layer1 = board[0].copy()\n",
    "    for c in range(0, columns):\n",
    "        for r in range(rows - 1, -1, -1):\n",
    "            value = layer1[r, c]\n",
    "            if value == 1:\n",
    "                layer1[r, c] = 1\n",
    "            else:\n",
    "                layer1[r, c] = 0\n",
    "\n",
    "    layer2 = board[0].copy()\n",
    "    for c in range(0, columns):\n",
    "        for r in range(rows - 1, -1, -1):\n",
    "            value = layer2[r, c]\n",
    "            if value == 2:\n",
    "                layer2[r, c] = 1\n",
    "            else:\n",
    "                layer2[r, c] = 0\n",
    "\n",
    "    layer3 = board[0].copy()\n",
    "    for c in range(0, columns):\n",
    "        for r in range(rows - 1, -1, -1):\n",
    "            value = layer3[r, c]\n",
    "            if value == 0:\n",
    "                if (mark == 1):\n",
    "                    layer3[r, c] = 1\n",
    "                else:\n",
    "                    layer3[r, c] = -1\n",
    "                break\n",
    "            else:\n",
    "                layer3[r, c] = 0\n",
    "\n",
    "    board = np.array([[layer1, layer2, layer3]])\n",
    "    return board\n",
    "\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    \"\"\"\n",
    "    Returns agent1's win percentage\n",
    "    \"\"\"\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 4))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 4))\n",
    "    print(\"Percentage of Invalid Plays by Agent 1:\", int(outcomes.count([None, 0])/n_rounds*100))\n",
    "    print(\"Percentage of Invalid Plays by Agent 2:\", int(outcomes.count([0, None])/n_rounds*100))\n",
    "    \n",
    "def agent(obs, config):\n",
    "    board_2d = np.array(obs['board']).reshape(1,6,7)\n",
    "    board_3c = transform_board(board_2d, obs.mark)\n",
    "    col, _ = model.predict(board_3c, deterministic=True)\n",
    "    return int(col)\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO CNN vs random (Stable Baselines)\n",
    "\n",
    "Trained by playing only vs the built-in random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.969\n",
      "Agent 2 Win Percentage: 0.028\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(os.path.join(MODEL_DIR, 'ppo_cnn_vs_random'))\n",
    "get_win_percentages(agent1=agent, agent2=\"random\", n_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN CNN self-play (Stable Baselines)\n",
    "\n",
    "Somewhat better. Still not 100% vs random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.966\n",
      "Agent 2 Win Percentage: 0.033\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(os.path.join(MODEL_DIR, 'dqn_cnn_self_play'))\n",
    "get_win_percentages(agent1=agent, agent2=\"random\", n_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN One Step Negamax Q Learning (From scratch in PyTorch)\n",
    "\n",
    "Solution inspired by https://www.kaggle.com/c/connectx/discussion/129145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "osnql_agent = AgentDQN()\n",
    "osnql_agent.load_policy_net('OneStepNegamaxQLearning/policy_net.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.99\n",
      "Agent 2 Win Percentage: 0.004\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(osnql_agent.kaggle_agent, 'random', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh. Still not 100%. So close though. Maybe random played 4 games really well? Let's try 10 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.9883\n",
      "Agent 2 Win Percentage: 0.0065\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(osnql_agent.kaggle_agent, 'random', 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. There are many ways to improve this solution:\n",
    "\n",
    "1) Look N steps into the future instead of 1 when calculating expected_state_action_values\n",
    "\n",
    "2) Train vs a fixed opponent\n",
    "\n",
    "3) Hardcode legal moves\n",
    "\n",
    "4) Add MCTS(this likely creates a near-perfect solution)\n",
    "\n",
    "5) Optimize hyperparameters\n",
    "\n",
    "...\n",
    "\n",
    "But I'd like to show you something else. I trained 2 more models with the identical settings. Let's see how well they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 2\n",
      "Agent 1 Win Percentage: 0.993\n",
      "Agent 2 Win Percentage: 0.0032\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n",
      "Agent 3\n",
      "Agent 1 Win Percentage: 0.992\n",
      "Agent 2 Win Percentage: 0.0042\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "osnql_agent2 = AgentDQN()\n",
    "osnql_agent2.load_policy_net('OneStepNegamaxQLearning/policy_net2.pt')\n",
    "osnql_agent3 = AgentDQN()\n",
    "osnql_agent3.load_policy_net('OneStepNegamaxQLearning/policy_net3.pt')\n",
    "\n",
    "print('Agent 2')\n",
    "get_win_percentages(osnql_agent2.kaggle_agent, 'random', 10_000)\n",
    "print('Agent 3')\n",
    "get_win_percentages(osnql_agent3.kaggle_agent, 'random', 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a new agent, which takes outputs from all 3 agents and sums them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_musketeers(obs, config):\n",
    "    state = osnql_agent.get_state(obs, obs.mark)\n",
    "    q1 = osnql_agent.policy(state)\n",
    "    q2 = osnql_agent2.policy(state)\n",
    "    q3 = osnql_agent3.policy(state)\n",
    "    q_c = q1 + q2 + q3\n",
    "    return q_c.max(1)[1].view(1, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.9989\n",
      "Agent 2 Win Percentage: 0.0008\n",
      "Percentage of Invalid Plays by Agent 1: 0\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(three_musketeers, 'random', 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Perfect result. It won all 10 000 games. Same test vs the built-in negamax agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1\n",
      "Agent 1 Win Percentage: 0.61\n",
      "Agent 2 Win Percentage: 0.26\n",
      "Percentage of Invalid Plays by Agent 1: 3\n",
      "Percentage of Invalid Plays by Agent 2: 0\n",
      "\n",
      "Agent 2\n",
      "Agent 1 Win Percentage: 0.6\n",
      "Agent 2 Win Percentage: 0.3\n",
      "Percentage of Invalid Plays by Agent 1: 2\n",
      "Percentage of Invalid Plays by Agent 2: 0\n",
      "\n",
      "Agent 3\n",
      "Agent 1 Win Percentage: 0.56\n",
      "Agent 2 Win Percentage: 0.32\n",
      "Percentage of Invalid Plays by Agent 1: 6\n",
      "Percentage of Invalid Plays by Agent 2: 0\n",
      "\n",
      "Combined\n",
      "Agent 1 Win Percentage: 0.71\n",
      "Agent 2 Win Percentage: 0.22\n",
      "Percentage of Invalid Plays by Agent 1: 2\n",
      "Percentage of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "print('Agent 1')\n",
    "get_win_percentages(osnql_agent.kaggle_agent, 'negamax', 100)\n",
    "print('\\nAgent 2')\n",
    "get_win_percentages(osnql_agent2.kaggle_agent, 'negamax', 100)\n",
    "print('\\nAgent 3')\n",
    "get_win_percentages(osnql_agent3.kaggle_agent, 'negamax', 100)\n",
    "print('\\nCombined')\n",
    "get_win_percentages(three_musketeers, 'negamax', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:connectx_env] *",
   "language": "python",
   "name": "conda-env-connectx_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
