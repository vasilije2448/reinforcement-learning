{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback, CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = os.path.join(\".\",\"logs\",\"custom_ppo_1\")\n",
    "MONITOR_LOGS_DIR = os.path.join(LOGDIR,\"monitor_logs\")\n",
    "TB_LOGS_DIR = os.path.join(LOGDIR,\"tensorboard_logs\")\n",
    "MODEL_DIR = os.path.join(LOGDIR,\"model\")\n",
    "CHECKPOINTS_DIR = os.path.join(LOGDIR,\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2=\"random\", warmup_episode_count = 100, warmup_timesteps = 5000):\n",
    "        self.env  = make(\"connectx\", debug=True)\n",
    "        self.trainer = self.env.train([None, agent2])\n",
    "        self.rows = self.env.configuration.rows\n",
    "        self.columns = self.env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "        \n",
    "        self.episode_count = 0\n",
    "        self.timesteps = 0\n",
    "        self.warmup_episode_count = warmup_episode_count\n",
    "        self.warmup_timesteps = warmup_timesteps\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_count += 1\n",
    "        self.obs = self.trainer.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        #print('episode count: ' + str(self.episode_count))\n",
    "        #print('timesteps: ' + str(self.timesteps))\n",
    "        self.timesteps += 1\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.trainer.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "    \n",
    "    def load_new_opponents_from_best_model(self):\n",
    "        if self.episode_count < self.warmup_episode_count or self.timesteps < self.warmup_timesteps:\n",
    "            return True\n",
    "        print(\"Loading new opponent from current best model for self play!!!\")\n",
    "        loaded_model = PPO.load(os.path.join(MODEL_DIR, \"best_model\")) \n",
    "        \n",
    "        def agent_ppo(obs, config):\n",
    "            # Use the best model to select a column\n",
    "            col, _ = loaded_model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "            # Check if selected column is valid\n",
    "            is_valid = (obs['board'][int(col)] == 0)\n",
    "            # If not valid, select random move. \n",
    "            if is_valid:\n",
    "                return int(col)\n",
    "            else:\n",
    "                return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n",
    "        \n",
    "        self.agent2 = agent_ppo\n",
    "        self.trainer = self.env.train([None, self.agent2])\n",
    "        self.reset()\n",
    "\n",
    "class LoadNewOpponentsFromBestModelCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose: int = 0):\n",
    "        super(LoadNewOpponentsFromBestModelCallback, self).__init__(verbose=verbose)\n",
    "        self.env = env\n",
    "\n",
    "    def _on_step(self):\n",
    "        env.load_new_opponents_from_best_model()\n",
    "        return True\n",
    "    \n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(ConnectFourGym())\n",
    "eval_env = DummyVecEnv([lambda:Monitor(ConnectFourGym())])\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=CHECKPOINTS_DIR, name_prefix=\"rl_model\")\n",
    "load_new_opponents_from_best_model_callback = LoadNewOpponentsFromBestModelCallback(env)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=MODEL_DIR,\n",
    "                             log_path=LOGDIR, eval_freq=20,\n",
    "                             deterministic=True, render=False\n",
    "                            , callback_on_new_best=load_new_opponents_from_best_model_callback, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "Loading new opponent from current best model for self play!!!\n",
      "training took -27.236652890841167 minutes\n"
     ]
    }
   ],
   "source": [
    "model = PPO(policy = 'MlpPolicy'\n",
    "                , env = env\n",
    "                , verbose = 0\n",
    "                , n_steps = 2048*16\n",
    "                , batch_size = 128\n",
    "                , n_epochs = 50\n",
    "                , tensorboard_log = TB_LOGS_DIR\n",
    "                , learning_rate = .01)\n",
    "\n",
    "start_time = time.time()\n",
    "model.learn(total_timesteps=100000, callback=[checkpoint_callback, eval_callback])\n",
    "end_time = time.time()\n",
    "print('training took ' + str((end_time - start_time)/60) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent1(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1), deterministic=True)\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_env = make(\"connectx\")\n",
    "#test_env.play([None, agent1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"random\", n_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.58\n",
      "Agent 2 Win Percentage: 0.42\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"random\", n_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's barely better than random. Sad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:connectx_env] *",
   "language": "python",
   "name": "conda-env-connectx_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
